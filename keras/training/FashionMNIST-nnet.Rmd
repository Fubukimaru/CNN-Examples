---
jupyter:
  jupytext:
    formats: ipynb,Rmd:rmarkdown
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 0.8.5
  kernelspec:
    display_name: R
    language: R
    name: ir
  language_info:
    codemirror_mode: r
    file_extension: .r
    mimetype: text/x-r-source
    name: R
    pygments_lexer: r
    version: 3.5.1
---

```{r}
library(keras)  # FashionMNIST dataset
library(nnet)  # Neural networks
library(caret)  # Cross Validation - loads nnet directly on trainControl

library(doMC)   # Parallel cross-validation
```

# Dataset


First we load the dataset from keras package. _Check legacyLoad.R to see how to load the dataset without using the package._

```{r}
fashion <- dataset_fashion_mnist()
```

```{r}
str(fashion)
attach(fashion) # So we can access test and train directly!
```

We get the following structure:

- train: Training dataset
    + x: the predictors, 28x28 pixels image in grayscale.
    + y: the response
- test: Testing datset (with x and y)

We can see the images with the following function:

```{r}
rotate <- function(x) t(apply(x, 2, rev))
show_image <- function(imgarray, col=gray(12:1/12), ...) {
  image(rotate(matrix(imgarray, nrow=28)), col=col, ...)
}

show_image(train$x[2,,])
show_image(train$x[10,,])
```

Notice that in y we have an integer from 0 to 9 (10 classes). They are in fact the following:
- 0: T-shirt/top
- 1: Trouser
- 2: Pullover
- 3: Dress
- 4: Coat
- 5: Sandal
- 6: Shirt
- 7: Sneaker
- 8: Bag
- 9: Ankle boot

We recode the response variable to factor.

```{r}
classString <- c("T-shirt/top","Trouser", "Pullover", "Dress", "Coat", "Sandal",
              "Shirt","Sneaker", "Bag","Ankle boot")

# y+1 because 0 is the first class and in R we start indexing at 1!
train$yFactor <- as.factor(classString[train$y+1]) 
test$yFactor <- as.factor(classString[test$y+1])
```

Now we prepare join the X and the Y in a data.frame.

```{r}
# Prepare dataset
nnetData <- data.frame(train$x, class=train$yFactor)
```

# Training


We can train the model directly as follows, but we will use _caret's_
_trainControl_ for CrossValidation.

```{r}
# model.nnet <- nnet(class ~ ., data=sub, size=50, maxit=300,decay=0.5, MaxNWts = 39760)
```

Specifically a 5 fold cross-validation. We don't go for a 10 fold
cross-validation as it will take a lot of time to compute.

```{r}
## specify 5-CV
K <- 5
trc <- trainControl (method="repeatedcv", number=K, repeats=1)
(decays <- 10^seq(-3,0,by=0.25))
```
We now specify that we want to execute the cross validation in parallel:

```{r}
# Use all cores except one (recommended if you want to use your computer for something else)
cores <- min(detectCores()-1, ceiling(length(decays)/2))
registerDoMC(cores = cores) 

```
Beware with the number of cores used, it will impact in the RAM usage. ~10GB per thread with 60K Fashion MNIST samples.

The cross-validation process will take about 30 hours using 7 cores of a Intel(R) 
Xeon(R) CPU E5-2630 v4 @ 2.20GHz and about 80 GB of RAM.

Remember that we're training (number_of_decay_param x number_of_folds) = 14 x 5
= 70 models.

```{r}
## WARNING: this takes some time
model.5CV <- train (class ~ ., data=nnetData, method='nnet', maxit = 300, trace = FALSE,
                      tuneGrid = expand.grid(.size=50,.decay=decays), trControl=trc, MaxNWts=39760)
```

```{r}
# Save model
save(model.5CV, file="nnet.mod")
```

```{r}
load("nnet.mod")
```

```{r}
model.5CV
```

The best model we got has an accuracy of 84%. Not bad at all for a 10 class
classificatin problem.


