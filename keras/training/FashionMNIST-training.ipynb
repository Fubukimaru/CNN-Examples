{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)  # FashionMNIST dataset\n",
    "library(nnet)  # Neural networks\n",
    "library(caret)  # Cross Validation - loads nnet directly on trainControl\n",
    "\n",
    "library(doParallel) # Parallel cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the dataset from keras package. _Check legacyLoad.R to see how to load the dataset without using the package._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion <- dataset_fashion_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(fashion)\n",
    "attach(fashion) # So we can access test and train directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following structure:\n",
    "\n",
    "- train: Training dataset\n",
    "    + x: the predictors, 28x28 pixels image in grayscale.\n",
    "    + y: the response\n",
    "- test: Testing datset (with x and y)\n",
    "\n",
    "We can see the images with the following function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate <- function(x) t(apply(x, 2, rev))\n",
    "show_image <- function(imgarray, col=gray(12:1/12), ...) {\n",
    "  image(rotate(matrix(imgarray, nrow=28)), col=col, ...)\n",
    "}\n",
    "\n",
    "show_image(train$x[2,,])\n",
    "show_image(train$x[10,,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response reencode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in y we have an integer from 0 to 9 (10 classes). They are in fact the following:\n",
    "- 0: T-shirt/top\n",
    "- 1: Trouser\n",
    "- 2: Pullover\n",
    "- 3: Dress\n",
    "- 4: Coat\n",
    "- 5: Sandal\n",
    "- 6: Shirt\n",
    "- 7: Sneaker\n",
    "- 8: Bag\n",
    "- 9: Ankle boot\n",
    "\n",
    "We recode the response variable to factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classString <- c(\"T-shirt/top\",\"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\",\n",
    "              \"Shirt\",\"Sneaker\", \"Bag\",\"Ankle boot\")\n",
    "\n",
    "# y+1 because 0 is the first class and in R we start indexing at 1!\n",
    "train$yFactor <- as.factor(classString[train$y+1]) \n",
    "test$yFactor <- as.factor(classString[test$y+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CNN we use one hot encoding to produce a vector of 10 values per sample, with a one on the class (probablity of belonging to a given class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train$yOneHot <- class.ind(train$yFactor)\n",
    "test$yOneHot <- class.ind(test$yFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(train$yOneHot)\n",
    "train$y[1:10]\n",
    "train$yOneHot[1:10,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*class.ind* reorders the classes alfabetically, therefore we need to revert this order to the original provided. We use *match* over the column names to get a vector of the reorder to match the column names to **classString**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames(train$yOneHot)\n",
    "classString\n",
    "(m <- match(classString, colnames(test$yOneHot))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train$yOneHot <- train$yOneHot[,m]\n",
    "test$yOneHot <- test$yOneHot[,m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the order is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames(train$yOneHot)\n",
    "classString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add missing dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers will expect the input to have 4 dimensions:\n",
    "- Sample dimension\n",
    "- Height dimension\n",
    "- Width dimension\n",
    "- Channel dimension\n",
    "\n",
    "In our case we have only one channel as the image is grayscale. If it's a color image we would have 3 or 4 channels (Red, Green, Blue and Alpha (transparency)). We need to add the missing dimension, however this will not modify the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(train$x) <- c(dim(train$x),1)\n",
    "dim(test$x) <- c(dim(test$x),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset for nnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare join the X and the Y in a data.frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnetData <- data.frame(train$x, class=train$yFactor)\n",
    "nnetDataTest <- data.frame(test$x, class=test$yFactor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the model directly as follows, but we will use _caret's_\n",
    "_trainControl_ for CrossValidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.nnet <- nnet(class ~ ., data=nnetData, size=50, maxit=300,decay=0.5, MaxNWts = 39760)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically a 5 fold cross-validation. We don't go for a 10 fold\n",
    "cross-validation as it will take a lot of time to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "## specify 5-CV\n",
    "K <- 5\n",
    "trc <- trainControl (method=\"repeatedcv\", number=K, repeats=1)\n",
    "(decays <- 10^seq(-3,0,by=0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now specify that we want to execute the cross validation in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Use all cores except one (recommended if you want to use your computer for something else). Or half of the lenght of the decays (RAM issues).\n",
    "cores <- min(detectCores()-1, ceiling(length(decays)/2))\n",
    "registerDoParallel(cores = cores) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware with the number of cores used, it will impact in the RAM usage. ~10GB per thread with 60K Fashion MNIST samples. **Don't execute this training if you don't have a big machine, just load the model.**\n",
    "\n",
    "The cross-validation process will take about 30 hours using 7 cores of a Intel(R) \n",
    "Xeon(R) CPU E5-2630 v4 @ 2.20GHz and about 80 GB of RAM.\n",
    "\n",
    "Remember that we're training (number_of_decay_param x number_of_folds) = 14 x 5\n",
    "= 70 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: this takes some time\n",
    "model.5CV <- train (class ~ ., data=nnetData, method='nnet', maxit = 300, trace = FALSE,\n",
    "                      tuneGrid = expand.grid(.size=50,.decay=decays), trControl=trc, MaxNWts=39760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save(model.5CV, file=\"nnet.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(\"nnet.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.5CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model we got has an accuracy of 84%. Not bad at all for a 10 class\n",
    "classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred <- predict(model.5CV,nnetDataTest)\n",
    "(t <- table(nnetDataTest$class, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "(accuracy <- sum(diag(t))/sum(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test also gives us 84% of accuracy. Nice model :)\n",
    "\n",
    "# Convolutional Neural Networks\n",
    "\n",
    "## Model architecture definition: LeNet\n",
    "\n",
    "Now we have to define the CNN architecture. In this case we use LeNet, proposed\n",
    "by LeCun et al. (Gradient-based learning applied to document recognition. \n",
    "Proceedings of the IEEE, november 1998). \n",
    "\n",
    "It is composed by two packs of convolutional-activation(tanh)-pooling layers and\n",
    "two fully connected layers with a softmax layer at the end.\n",
    "\n",
    "In Keras, as in most of the packages, we define layers as objects and the \n",
    "connections between those objects. In this case we implicitly connect everything\n",
    "using the %>% operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet <- keras_model_sequential() %>%\n",
    "    # First convolutional block\n",
    "    layer_conv_2d(filters=20, kernel_size=c(5,5), activation=\"tanh\",\n",
    "        input_shape=c(28,28,1), padding=\"same\") %>%  # We define here the input size\n",
    "    layer_max_pooling_2d(pool_size=c(2,2),strides=c(2,2)) %>%\n",
    "    # Second convolutional block\n",
    "    layer_conv_2d(filters=50, kernel_size=c(5,5), activation=\"tanh\",\n",
    "        input_shape=c(28,28,1), padding=\"same\") %>% \n",
    "    layer_max_pooling_2d(pool_size=c(2,2),strides=c(2,2)) %>%\n",
    "    \n",
    "    # Flatten the matrix to a vector for the fully connected layers\n",
    "    layer_flatten() %>%\n",
    "\n",
    "    # First fully connected block\n",
    "    layer_dense(units=500, activation=\"tanh\") %>%\n",
    "    # Second fully connected block\n",
    "    layer_dense(units=10, activation=\"softmax\") \n",
    "    # This last layer will produce the final classification (probability of \n",
    "    # belonging to a class). 10 different units, 10 different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we check the architecture we have defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're adjusting 1 million parameters this time. With the nnet one layer network we were training just 39.760 parameters.\n",
    "\n",
    "Last thing we have to do is to specify which optimizaton algorithm and metrics\n",
    "we want to use with the compile step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd <- optimizer_sgd(\n",
    "                     lr=0.05, \n",
    "                     decay=0.001, \n",
    "                     momentum=0.8, \n",
    "                     clipnorm=1.\n",
    ")\n",
    "lenet %>% compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics = \"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Now we're going to train the network using CPU (if you're not using\n",
    "tensorflow-gpu). Mind that if you want to use GPUs you need to have the GPU \n",
    "version of the package and the required Nvidia packages (check PlaidML for \n",
    "non-Nvidia GPUs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet %>% fit(\n",
    "          train$x, \n",
    "          train$yOneHot, \n",
    "          batch_size=50,\n",
    "          validation_split=0.2,\n",
    "          epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes about 3 minuts with 40 cores and 18 GB of RAM. It may take less with GPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "And now we save the trained model for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet %>% save_model_hdf5(\"lenet-FashionMNIST.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using the model\n",
    "\n",
    "Predicting the label for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet <- load_model_hdf5(\"lenet-FashionMNIST.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob <- predict(lenet, test$x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each element we get the probability of that element to be of each class, therefore we search for the value that is maximum in each row and then we create the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predClass <- apply(pred_prob,1,which.max)\n",
    "predClass <- classString[predClass] # And change the integers by their class tag\n",
    "\n",
    "\n",
    "trueClass <- test$yFactor\n",
    "\n",
    "# Now we do a confusion matrix and analyze it\n",
    "(cMatrix <- table(trueClass,predClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctClass <- sum(diag(cMatrix))\n",
    "total <- sum(cMatrix)\n",
    "(accuracy <- correctClass/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting about a 90% of accuracy that may be improved with further tunning of the network. Notice that, for example, there are 30 ankle boots classified as sneakers, that can have similar shapes. Also there are 113 pullovers are classified as coats."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "209.667px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "threshold": 4,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
