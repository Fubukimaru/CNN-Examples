combined<- mx.symbol.Group(act1A, convnet)
executor <- mx.simple.bind(symbol=combined, data=dim(train_array), ctx=devices)
mx.set.seed(123)
model_mxnet <- mx.model.FeedForward.create(convnet,
X=train_array,
y=train_labels,,
eval.data = list(data=eval_array, label=eval_labels),
array.batch.size = 16,
ctx=devices,
num.round=5,
learning.rate=0.05,
wd=0.001,
momentum=0.9,
clip_gradient=1,
eval.metric=mx.metric.accuracy,
initializer=mx.init.Xavier(rnd_type = "gaussian", factor_type = "avg", magnitude = 3),
epoch.end.callback = mx.callback.log.train.metric(1))
for (i in 1:16) {
img_array <- as.array(executor$outputs$activation0_output)[,,i,1]
img<-as.cimg(img_array)
plot(img)
}
head(submit)
pred_prob<- t(predict(model_mxnet, eval_array))
submit <- data.frame(id=1:64, label=pred_prob[, 2])
head(submit)
?mx.model.FeedForward.create
library(mxnet)
?mx.model.FeedForward.create
library(imager)
train_path <- file.path(data_path_base,"train")
library(imager)
library(data.table)
library(dtplyr)
library(dplyr)
library(readr)
library(ggplot2)
library(plotly)
library(mxnet)
data_path_base <- "~/data/dogcats"
train_path <- file.path(data_path_base,"train")
test_path <- file.path(data_path_base,"test")
#### Create two lists, one for cats and other for dogs. This function uses a
####  pattern maching approach with the parameter "pattern" which has as input
####  a regular expression.
cat_files<- list.files(path = train_path, pattern = "cat.+")
dog_files<- list.files(path = train_path, pattern = "dog.+")
### Randomize image order
set.seed(123)
cat_files<- sample(cat_files)
dog_files<- sample(dog_files)
### List test files and order them by name
test_files<- list.files(path = test_path, pattern = ".+\\.jpg")
test_num<- as.numeric(gsub(pattern="(\\d+)(.+)", replacement = "\\1", x = test_files))
test_position<- order(test_num)
test_files<- test_files[test_position]
head(cat_files)
head(dog_files)
head(test_files)
### Load images and and convert to array
train_prep<- function(start,end, size){
count<- 2*length(start:end)
# Initialize array with NA ([Size x Size x 3 channels (RGB)] x number of img)
data_array<- array(NA, dim = c(size, size, 3, count))
for (i in start:end) {
# Load jpeg into imageR
cat <- load.image(file.path(train_path, cat_files[i]))
# Resize image to a 80x80 with 3 channels image
cat<- resize(cat,size_x = size, size_y = size, size_c = 3, interpolation_type = 2)
# Transform the image into a vector/array
cat_array<-t(round(array(cat),4))
dog <- load.image(file.path(train_path, dog_files[i]))
dog<- resize(dog,size_x = size, size_y = size, size_c = 3, interpolation_type = 2)
dog_array<-t(round(array(dog),4))
# Add images to the result
data_array[, , , 2*(i-start+1)-1]<- cat_array
data_array[, , , 2*(i-start+1)]<- dog_array
}
return(data_array)
}
size <- 80 # In this case we want images of 80x80 pixels.
# Get the first 160 training images
train_array<- train_prep(start=1, end=160, size=size)
# Generate labels. As we are getting one cat and one dog, we can just repeat the
#  labels 0 (cat) and 1 (dog) 160 times. It will generate a vector of 160*2
#  positions.
train_labels<- rep(c(0,1), times=160)
# Generate evaluation set.
eval_array<- train_prep(start=10001, end=10032, size=size)
eval_labels<- rep(c(0,1), times=32)
# Get first cat cat
img_test <- train_array[, , , 1]
dim(img_test) <- c(size, size, 1, 3)
img_test <- as.cimg(img_test)
plot(img_test)
# Get the first dog
img_test <- train_array[, , , 2]
dim(img_test) <- c(size, size, 1, 3)
img_test <- as.cimg(img_test)
plot(img_test)
# input
data <- mx.symbol.Variable('data')
conv1A <- mx.symbol.Convolution(data=data, kernel=c(3,3), num_filter=32, stride=c(1,1), pad=c(1,1))
act1A <- mx.symbol.Activation(data=conv1A, act_type="relu")
conv1B <- mx.symbol.Convolution(data=act1A, kernel=c(3,3), num_filter=32, stride=c(1,1), pad=c(1,1))
act1B <- mx.symbol.Activation(data=conv1B, act_type="relu")
pool1<- mx.symbol.Pooling(data=act1B, pool_type="max", kernel=c(2,2), stride=c(2,2), pad=c(0,0))
conv2A <- mx.symbol.Convolution(data=pool1, kernel=c(3,3), num_filter=64, stride=c(1,1), pad=c(1,1))
act2A <- mx.symbol.Activation(data=conv2A, act_type="relu")
conv2B <- mx.symbol.Convolution(data=act2A, kernel=c(3,3), num_filter=64, stride=c(1,1), pad=c(1,1))
act2B <- mx.symbol.Activation(data=conv2B, act_type="relu")
pool2<- mx.symbol.Pooling(data=act2B, pool_type="max", kernel=c(2,2), stride=c(2,2), pad=c(0,0))
conv3A <- mx.symbol.Convolution(data=pool2, kernel=c(3,3), num_filter=128, stride=c(1,1), pad=c(1,1))
act3A <- mx.symbol.Activation(data=conv3A, act_type="relu")
conv3B <- mx.symbol.Convolution(data=act3A, kernel=c(3,3), num_filter=128, stride=c(1,1), pad=c(1,1))
act3B <- mx.symbol.Activation(data=conv3B, act_type="relu")
pool3<- mx.symbol.Pooling(data=act3B, pool_type="max", kernel=c(2,2), stride=c(2,2), pad=c(0,0))
conv4A <- mx.symbol.Convolution(data=pool3, kernel=c(3,3), num_filter=128, stride=c(1,1), pad=c(1,1))
act4A <- mx.symbol.Activation(data=conv4A, act_type="relu")
conv4B <- mx.symbol.Convolution(data=act4A, kernel=c(3,3), num_filter=128, stride=c(1,1), pad=c(1,1))
act4B <- mx.symbol.Activation(data=conv4B, act_type="relu")
pool4<- mx.symbol.Pooling(data=act4B, pool_type="max", kernel=c(2,2), stride=c(2,2), pad=c(0,0))
# flatten
flatten <- mx.symbol.Flatten(data=pool4)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=512)
act_fc1 <- mx.symbol.Activation(data=fc1, act_type="relu")
drop1 <- mx.symbol.Dropout(data=act_fc1, p=0.5)
# loss
fc_final <- mx.symbol.FullyConnected(data=drop1, num_hidden=2)
convnet <- mx.symbol.SoftmaxOutput(data=fc_final)
### plot the model structure
graph.viz(convnet)
devices <- mx.cpu()
### combine symbols and create executor for inspection of learned features
combined<- mx.symbol.Group(act1A, convnet)
executor <- mx.simple.bind(symbol=combined, data=dim(train_array), ctx=devices)
mx.set.seed(123)
model_mxnet <- mx.model.FeedForward.create(convnet,
X=train_array,
y=train_labels,
eval.data = list(data=eval_array, label=eval_labels),
array.batch.size = 16,
ctx=devices,
num.round=5,
learning.rate=0.05,
wd=0.001,
momentum=0.9,
clip_gradient=1,
eval.metric=mx.metric.accuracy,
initializer=mx.init.Xavier(rnd_type = "gaussian", factor_type = "avg", magnitude = 3),
epoch.end.callback = mx.callback.log.train.metric(1))
img_array <- as.array(executor$outputs$activation0_output)[,,i,1]
mx.exec.update.arg.arrays(exec = executor, arg.arrays = model_mxnet$arg.params, match.name=TRUE)
mx.exec.update.arg.arrays(executor, list(data=mx.nd.array(train_array)), match.name=TRUE)
mx.exec.forward(executor, is.train=FALSE)
par(mfrow=c(4,4), mar=c(0.1,0.1,0.1,0.1))
for (i in 1:16) {
img_array <- as.array(executor$outputs$activation0_output)[,,i,1]
img<-as.cimg(img_array)
plot(img)
}
pred_prob<- t(predict(model_mxnet, eval_array))
submit <- data.frame(id=1:64, label=pred_prob[, 2])
head(submit)
yatch <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data", sep=" ", header = FALSE)
yatch <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data", sep=" ", dec = ".", header = FALSE)
yatch <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data",
sep="\t", dec = ".", header = FALSE)
dim(yatch)
yatch$V1
yatch <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data",
sep="", dec = ".", header = FALSE)
dim(yatch)
names(yatch) <- c("buoyancyPos", "prismaticCoef", "lengthDispRatio",
"beamDraughtRatio", "lengthBeanRatio", "froudeNo.")
names(yatch) <- c("buoyancyPos", "prismaticCoef", "lengthDispRatio",
"beamDraughtRatio", "lengthBeanRatio", "froudeNo.",
"residuaryResistance")
yatch$residuaryResistance
write.csv(yatch, file="/tmp/yatch.csv")
write.csv(yatch, file="/tmp/yatch.csv", row.names = FALSE)
write.csv(yatch, file="/tmp/yatch.csv", row.names = FALSE, sep=";")
write.csv(yatch, file="/tmp/yatch.csv", row.names = FALSE)
yacht <- read.table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data"),
header=FALSE,
col.names = c("Longitudinal.pos","Prismatic.coeff","Length.displ","Beam.draught
","Length.beam","Froude.No", "Resistance"))
yatch$residuaryResistance
write.csv(yacht, file="/home/fubu/Dropbox/Work/UPC School/2017-2018-5th/Lluis/PrepMaths/2/yacht.csv", row.names = FALSE)
library(imager)
library(data.table)
library(dtplyr)
library(dplyr)
library(readr)
library(ggplot2)
library(plotly)
library(mxnet)
data_path_base <- "~/data/dogcats"
train_path <- file.path(data_path_base,"train")
test_path <- file.path(data_path_base,"test")
#### Create two lists, one for cats and other for dogs. This function uses a
####  pattern maching approach with the parameter "pattern" which has as input
####  a regular expression.
cat_files<- list.files(path = train_path, pattern = "cat.+")
dog_files<- list.files(path = train_path, pattern = "dog.+")
### Randomize image order
set.seed(123)
cat_files<- sample(cat_files)
dog_files<- sample(dog_files)
### List test files and order them by name
test_files<- list.files(path = test_path, pattern = ".+\\.jpg")
test_num<- as.numeric(gsub(pattern="(\\d+)(.+)", replacement = "\\1", x = test_files))
test_position<- order(test_num)
test_files<- test_files[test_position]
head(cat_files)
head(dog_files)
head(test_files)
### Load images and and convert to array
train_prep<- function(start,end, size){
count<- 2*length(start:end)
# Initialize array with NA ([Size x Size x 3 channels (RGB)] x number of img)
data_array<- array(NA, dim = c(size, size, 3, count))
for (i in start:end) {
# Load jpeg into imageR
cat <- load.image(file.path(train_path, cat_files[i]))
# Resize image to a 80x80 with 3 channels image
cat<- resize(cat,size_x = size, size_y = size, size_c = 3, interpolation_type = 2)
# Transform the image into a vector/array
cat_array<-t(round(array(cat),4))
dog <- load.image(file.path(train_path, dog_files[i]))
dog<- resize(dog,size_x = size, size_y = size, size_c = 3, interpolation_type = 2)
dog_array<-t(round(array(dog),4))
# Add images to the result
data_array[, , , 2*(i-start+1)-1]<- cat_array
data_array[, , , 2*(i-start+1)]<- dog_array
}
return(data_array)
}
size <- 80 # In this case we want images of 80x80 pixels.
# Get the first 160 training images
train_array<- train_prep(start=1, end=160, size=size)
# Generate labels. As we are getting one cat and one dog, we can just repeat the
#  labels 0 (cat) and 1 (dog) 160 times. It will generate a vector of 160*2
#  positions.
train_labels<- rep(c(0,1), times=160)
# Generate evaluation set.
eval_array<- train_prep(start=10001, end=10032, size=size)
eval_labels<- rep(c(0,1), times=32)
dim(train_array)
?mx.simple.bind
train_array
dim(train_array)
dim(train_array)
library(knitr)
purl("CNN-Initial.Rmd", output = "CNN-Initial.R", documentation = 2)
library(mxnet)
library(imager)
model = mx.model.load("Inception/Inception_BN", iteration=39)
setwd("~/workspace/CNN-Examples/pretrained")
model = mx.model.load("Inception/Inception_BN", iteration=39)
# Load average image (of all the imaged used for training)
mean.img <- as.array(mx.nd.load("Inception/mean_224.nd")[["mean_img"]])
plot(as.cimg(mean.img))
plot(as.cimg(mean.img))
# Now we get the class names
synsets <- readLines("Inception/synset.txt")
length(synsets)
head(synsets)
# Image preprocess
preproc.image <- function(im, mean.image, crop = TRUE, dims=3) {
if (crop) {
## Crop the image so it gets same height and width
shape <- dim(im)
short.edge <- min(shape[1:2]) # Get the shorter edge from the picture
# Calculate how much we should crop for each axis
xx <- floor((shape[1] - short.edge) / 2)
yy <- floor((shape[2] - short.edge) / 2)
im <- crop.borders(im, xx, yy) # Cropped image
}
# Resize to 224 x 224, needed by input of the model.
resized <- resize(im, 224, 224)
# Convert to array (x, y, channel)
arr <- as.array(resized) * 255 # From 0..1 to 0..255 (RGB integer codification)
dim(arr) <- c(224, 224, 3)
# Subtract the mean
preproc <- arr - mean.img
# Reshape to format needed by mxnet (width, height, channel, num)
dim(preproc) <- c(224, 224, 3, 1)
return(preproc)
}
# Result printing
printClassRank <- function(prob, labels, nRes = 10) {
nRes <- min(nRes, length(labels))
o <- order(prob, decreasing=TRUE)
res <- data.frame(class=synsets[o], probability=prob[o])
head(res, n = nRes)
}
######## Starters: Take an image from imageR package - Give me macaws! #########
im <- load.image(system.file("extdata/parrots.png", package="imager"))
plot(im)
dim(im)
# Preprocess the parrots
preproc <- preproc.image(im, mean.img)
plot(as.cimg(preproc[,,,1]))
par(mfrow=c(1,2))
plot(im)
plot(as.cimg(preproc[,,,1]))
# Predict the parrots! In other words, get the class probabilities
prob <- predict(model, X=preproc)
# Preprocess the parrots
preproc <- preproc.image(im, mean.img)
# Predict the parrots! In other words, get the class probabilities
prob <- predict(model, X=preproc)
dim(prob)
# Which classes ar the most representative
printClassRank(prob, synsets)
##### Now with something completely different: A laptop with Alpha Channel #####
# Load new image
im2 <- load.image("images/laptop.png")
plot(im2)
dim(im2)
# Remove Alpha Channel
im2 <- rm.alpha(im2)
plot(im2)
dim(im2)
# Preprocess the laptop
preproc <- preproc.image(im2, mean.img)
plot(im2); plot(as.cimg(preproc[,,,1]))
# Prediction again...
prob <- predict(model, X=preproc)
printClassRank(prob, synsets)
###### Picture proportion is important: Is the network aware of dinosaurs?######
# Load new image
im3 <- load.image("images/dinosaur.jpg")
plot(im3)
preproc <- preproc.image(im3, mean.img)
plot(as.cimg(preproc[,,,1]))
# The head gets cut, does it matter?
prob <- predict(model, X=preproc)
# Which class is the most representative
printClassRank(prob, synsets)
# What happens if we resize without cropping the image so that the head is
#  preserved?
preproc <- preproc.image(im3, mean.img, crop=FALSE)
plot(as.cimg(preproc[,,,1]))
prob <- predict(model, X=preproc)
# Which class is the most representative
printClassRank(prob, synsets)
# What if we center it manually?
im3.2 <- load.image("images/dinosaurSquare.jpg")
plot(im3.2)
preproc <- preproc.image(im3.2, mean.img)
plot(as.cimg(preproc[,,,1]))
prob <- predict(model, X=preproc)
# Which class is the most representative
printClassRank(prob, synsets)
################# Can the network identify Agbar Tower? ########################
# Load new image
im4 <- load.image("images/agbar.jpg")
plot(im4)
plot(im4)
preproc <- preproc.image(im4, mean.img)
plot(as.cimg(preproc[,,,1]))
prob <- predict(model, X=preproc)
# Which class is the most representative
printClassRank(prob, synsets)
# Let's try with another photo
im5 <- load.image("images/Agbar2.png")
plot(im5)
# Normalize the laptop
preproc <- preproc.image(im5, mean.img)
plot(as.cimg(preproc[,,,1]))
prob <- predict(model, X=preproc)
printClassRank(prob, synsets)
########## Identifying by aesthetics; You may know what is this one ############
im6 <- load.image("images/NES.png")
dim(im6)
# Notice that this image has 4 channels!
plot(im6)
# Remove Alpha Channel
im6 <- rm.alpha(im6)
preproc <- preproc.image(im6, mean.img)
plot(as.cimg(preproc[,,,1]))
prob <- predict(model, X=preproc)
printClassRank(prob, synsets)
unzip('fashionMNIST/train-images-idx3-ubyte.zip', exdir = "fashionMNIST")
unzip('fashionMNIST/t10k-images-idx3-ubyte.zip', exdir = "fashionMNIST")
setwd("~/workspace/CNN-Examples/training")
unzip('fashionMNIST/train-images-idx3-ubyte.zip', exdir = "fashionMNIST")
unzip('fashionMNIST/t10k-images-idx3-ubyte.zip', exdir = "fashionMNIST")
unzip('fashionMNIST/train-labels-idx1-ubyte.zip', exdir = "fashionMNIST")
unzip('fashionMNIST/t10k-labels-idx1-ubyte.zip', exdir = "fashionMNIST")
load_image_file <- function(filename) {
ret = list()
f = file(filename,'rb')
readBin(f,'integer',n=1,size=4,endian='big')
ret$n = readBin(f,'integer',n=1,size=4,endian='big')
nrow = readBin(f,'integer',n=1,size=4,endian='big')
ncol = readBin(f,'integer',n=1,size=4,endian='big')
x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
close(f)
ret
}
load_label_file <- function(filename) {
f = file(filename,'rb')
readBin(f,'integer',n=1,size=4,endian='big')
n = readBin(f,'integer',n=1,size=4,endian='big')
y = readBin(f,'integer',n=n,size=1,signed=F)
close(f)
y
}
# Loading train and test images
train <- load_image_file('fashionMNIST/train-images-idx3-ubyte')
test <- load_image_file('fashionMNIST/t10k-images-idx3-ubyte')
# Loading labels
train$y <- load_label_file('fashionMNIST/train-labels-idx1-ubyte')
test$y <- load_label_file('fashionMNIST/t10k-labels-idx1-ubyte')
# Create a factor label array
classString <- c("T-shirt/top","Trouser", "Pullover", "Dress", "Coat", "Sandal",
"Shirt","Sneaker", "Bag","Ankle boot")
train$yFactor <- as.factor(classString[train$y+1])
test$yFactor <- as.factor(classString[test$y+1])
# Now we can check the images check images
show_image <- function(imgarray, col=gray(12:1/12), ...) {
image(matrix(imgarray, nrow=28)[,28:1], col=col, ...)
}
oldpar <- par()
par(mfrow=c(3,3))
show_image(train$x[1,])
show_image(train$x[13,])
show_image(train$x[54,])
show_image(train$x[100,])
show_image(train$x[130,])
show_image(train$x[230,])
show_image(train$x[412,])
show_image(train$x[454,])
show_image(train$x[540,])
par(oldpar)
load("nnet-simple.mod")
library(nnet)
library(caret)
load("nnet-simple.mod")
predT <- predict(model.nnet.simple, newdata=test$x, type="class")
(tab <- table(Truth=test$yFactor, Pred=predT))
(sum(diag(tab))/sum(tab))*100
# Reshape data for CNN input
# --------------------------
#
# As input data we have now a matrix of N rows times M columns. The rows represent
# each image and the columns each pixel of the image. We will reshape it to take
# the shape of a 4 dimensional array. First identifier will be the image, second
# the channel and third and fourth will be the coordinates of a given pixel of the
# image, i.e. Width x Height x Channels x Images.
train$x <- array(train$x, c(train$n,1,28,28)) # Transform into Nx1x28x28 array
train$x <- aperm(train$x, c(3,4,2,1))         # Permutate columns into 28x28x1xN array
test$x <- array(test$x, c(test$n,1,28,28))
test$x <- aperm(test$x, c(3,4,2,1))
#
# Now we divide between training set and validation set. Mind that as we are working
# with a grayscale image we only have one channel. This is dangerous in R as if we
# subset the dataset using [] operator, R will remove the dimension that only has one identifier (channels). To prevent this we have to do as follows: x[i, j, drop = FALSE] instead of x[i,j].
#
valSamp <- sample(length(train$y)/3)
validation <- list(x = train$x[,,,valSamp, drop=FALSE], y= train$y[valSamp]) # drop=FALSE forbids R to drop dimensions with length 1
train$x <- train$x[,,,-valSamp, drop=FALSE]
train$y <- train$y[-valSamp]
valSamp <- sample(length(train$y)/3)
validation <- list(x = train$x[,,,valSamp, drop=FALSE], y= train$y[valSamp]) # drop=FALSE forbids R to drop dimensions with length 1
train$x <- train$x[,,,-valSamp, drop=FALSE]
train$y <- train$y[-valSamp]
data <- mx.symbol.Variable('data')
# first conv
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20)
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max",
kernel=c(2,2), stride=c(2,2))
# second conv
conv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max",
kernel=c(2,2), stride=c(2,2))
# first fullc
flatten <- mx.symbol.Flatten(data=pool2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
# second fullc
fc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
# loss
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
# And now we an visualize the model representation with the following commands:
graph.viz(lenet)
model.mxnet <- mx.model.load("mxnet.mod", iteration=50)
library(mxnet)
model.mxnet <- mx.model.load("mxnet.mod", iteration=50)
model.mxnet <- mx.model.load("mxnet.mod", iteration=50)
# Predicting the label for the test set
pred_prob<- t(predict(model.mxnet, test$x))
head(pred_prob)
# For each element we get the probability of that element to be of each class,
#  therefore we search for the value that is maximum in each row as follows:
predClass <- factor(apply(pred_prob,1,which.max))
levels(predClass) <- classString # Change integers to string representation
trueClass <- factor(test$y)
levels(trueClass) <- classString
# Now we do a confusion matrix and analyze it
(cMatrix <- table(trueClass,predClass))
correctClass <- sum(diag(cMatrix))
total <- sum(cMatrix)
(accuracy <- correctClass/total)
library(caret)
?trainControl
