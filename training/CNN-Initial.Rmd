---
title: "CNN Example - Dogs vs Cats"
output:
  pdf_document: default
  html_notebook: default
---
References:

- [MXNet CNN Starter kit](https://www.kaggle.com/jeremiedb/mxnet-with-r-starter-kit)
- [Kaggle: Cats vs Dogs dataset](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data)
- [Dataset Mirror](https://www.floydhub.com/fastai/datasets/kaggle-dogs-vs-cats-redux-kernels-edition)
- [R Markdown](http://rmarkdown.rstudio.com)


Setup:

```{r}
# install.packages(c("imager", "data.table", "dtplyr", "dplyr", "readr", "ggplot2", "plotly"))
```




```{r}
library(imager)
library(data.table)
library(dtplyr)
library(dplyr)
library(readr)
library(ggplot2)
library(plotly)
library(mxnet)
```
Input preprocessing
===================

Before starting we need to process the images, as they are not a standard 
data.frame. We first are required to build a list of filenames out of the images.

```{r}
# Load the MNIST digit recognition dataset into R
# http://yann.lecun.com/exdb/mnist/
# assume you have all 4 files and gunzip'd them
# creates train$n, train$x, train$y  and test$n, test$x, test$y
# e.g. train$x is a 60000 x 784 matrix, each row is one digit (28x28)
# call:  show_digit(train$x[5,])   to see a digit.
# brendan o'connor - gist.github.com/39760 - anyall.org

load_mnist <- function() {
  load_image_file <- function(filename) {
    ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
  }
  load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
  }
  train <<- load_image_file('mnist/train-images-idx3-ubyte')
  test <<- load_image_file('mnist/t10k-images-idx3-ubyte')
  
  train$y <<- load_label_file('mnist/train-labels-idx1-ubyte')
  test$y <<- load_label_file('mnist/t10k-labels-idx1-ubyte')  
}


show_digit <- function(arr784, col=gray(12:1/12), ...) {
  image(matrix(arr784, nrow=28)[,28:1], col=col, ...)
}
```

Image Loading
-------------

```{r}
  load_image_file <- function(filename) {
    ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
  }

  load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
  }
  train <- load_image_file('~/data/fashionMNIST/train-images-idx3-ubyte')
  test <- load_image_file('~/data/fashionMNIST/t10k-images-idx3-ubyte')
  
  train$y <- load_label_file('~/data/fashionMNIST/train-labels-idx1-ubyte')
  test$y <- load_label_file('~/data/fashionMNIST/t10k-labels-idx1-ubyte') 
```

Reshape data for CNN input
--------------------------

As input data we have now a matrix of N rows times M columns. The rows represent
each image and the columns each pixel of the image. We will reshape it to take
the shape of a 4 dimensional array. First identifier will be the image, second 
the channel and third and fourth will be the coordinates of a given pixel of the
image.

Mind that as we are working with a grayscale image we only have one channel.

```{r}
train$x <- array(train$x, c(train$n,1,28,28))
train$x <- aperm(train$x, c(3,4,2,1))
test$x <- array(test$x, c(test$n,1,28,28))
test$x <- aperm(test$x, c(3,4,2,1))
```



Check images after preprocess
-----------------------------
```{r}
show_image <- function(imgarray, col=gray(12:1/12), ...) {
  image(matrix(imgarray, nrow=28)[,28:1], col=col, ...)
}

show_image(train$x[,,,13])

```

Model architecture definition
-----------------------------

Now we have to define the CNN architecture.

LeNet
-----

```{r}
# Lenet

#input
data <- mx.symbol.Variable('data')
# first conv
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20)
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max",
                           kernel=c(2,2), stride=c(2,2))
# second conv
conv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max",
                           kernel=c(2,2), stride=c(2,2))
# first fullc
flatten <- mx.symbol.Flatten(data=pool2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
# second fullc
fc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
# loss
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
```



Model training
--------------

```{r}
devices <- mx.cpu()

### combine symbols and create executor for inspection of learned features
combined<- mx.symbol.Group(tanh1, lenet)
executor <- mx.simple.bind(symbol=combined, data=dim(train$x), ctx=devices)

mx.set.seed(123)
model_mxnet <- mx.model.FeedForward.create(lenet,
                                               X=train$x, 
                                               y=train$y,
                                               eval.data = list(data=test$x, label=test$y),
                                               array.batch.size = 100, 
                                               ctx=devices, 
                                               num.round=5,
                                               learning.rate=0.05,
                                               wd=0.001,
                                               momentum=0.1,
                                               clip_gradient=1,
                                               eval.metric=mx.metric.accuracy, 
                                               initializer=mx.init.Xavier(rnd_type = "gaussian", factor_type = "avg", magnitude = 3),
                                               epoch.end.callback = mx.callback.log.train.metric(1))

```
Start training with 1 devices
[1] Train-accuracy=0.754774624373956
[1] Validation-accuracy=0.794
[2] Train-accuracy=0.831833333333333
[2] Validation-accuracy=0.8396
[3] Train-accuracy=0.849433333333332
[3] Validation-accuracy=0.8543
[4] Train-accuracy=0.85745
[4] Validation-accuracy=0.8566
[5] Train-accuracy=0.8469
[5] Validation-accuracy=0.8194


Looking at generated features
-----------------------------
```{r}
mx.exec.update.arg.arrays(exec = executor, arg.arrays = model_mxnet$arg.params, match.name=TRUE)
mx.exec.update.arg.arrays(executor, list(data=mx.nd.array(train_array)), match.name=TRUE)
mx.exec.forward(executor, is.train=FALSE)

par(mfrow=c(4,4), mar=c(0.1,0.1,0.1,0.1))
for (i in 1:16) {
  img_array <- as.array(executor$outputs$activation0_output)[,,i,1]
  img<-as.cimg(img_array)
  plot(img)
}
```

Generating predictions
----------------------

```{r}
pred_prob<- t(predict(model_mxnet, test$x))
submit <- data.frame(id=1:64, label=pred_prob[, 2])
head(submit)
```

